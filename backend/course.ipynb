{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: PEH DO YOU WANT TO SPEAK ENGLISH FLUENTLY NOW GO TO POWER ENGLISH DATNETTE AND DOWNLOAD OUR IMPY THREE AUDIALE LESSONS LISTEN TO OUR EASY STORIES AND CONVERSATION LESSONS EVERY DAY YOU WILL LEARN ENGLISH VOCABULARY EASILY AND IMPROVE YOUR ENGLISH LISTENING AND SPEAKING SKILLS FAST\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base-960h\"):\n",
    "        \"\"\"\n",
    "        Initialise le modèle de transcription audio.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nom du modèle Wav2Vec2\n",
    "        \"\"\"\n",
    "        # Charger le processeur et le modèle\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "    \n",
    "    def transcribe_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        Transcrit un fichier audio en texte.\n",
    "        \n",
    "        Args:\n",
    "            audio_path (str): Chemin vers le fichier audio MP3\n",
    "        \n",
    "        Returns:\n",
    "            str: Transcription du texte\n",
    "        \"\"\"\n",
    "        # Charger l'audio\n",
    "        audio, _ = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Prétraitement de l'audio\n",
    "        input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        \n",
    "        # Désactiver le calcul de gradient\n",
    "        with torch.no_grad():\n",
    "            # Obtenir les logits de prédiction\n",
    "            logits = self.model(input_values).logits\n",
    "        \n",
    "        # Prédire les tokens\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Convertir en texte\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        return transcription\n",
    "\n",
    "def main():\n",
    "    # Utilisation\n",
    "    transcriber = AudioTranscriber()\n",
    "    transcription = transcriber.transcribe_audio(\"Audio/Download1.mp3\")\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
